{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:09:11.436397Z","iopub.execute_input":"2025-04-10T16:09:11.436718Z","iopub.status.idle":"2025-04-10T16:09:11.733812Z","shell.execute_reply.started":"2025-04-10T16:09:11.436691Z","shell.execute_reply":"2025-04-10T16:09:11.733193Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:09:13.577346Z","iopub.execute_input":"2025-04-10T16:09:13.577745Z","iopub.status.idle":"2025-04-10T16:09:13.798585Z","shell.execute_reply.started":"2025-04-10T16:09:13.577722Z","shell.execute_reply":"2025-04-10T16:09:13.797561Z"}},"outputs":[{"name":"stdout","text":"Thu Apr 10 16:09:13 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   43C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   43C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# !pip install  dgl==2.0.0 -f https://data.dgl.ai/wheels/repo.html\n# !pip install  dglgo==0.0.2 -f https://data.dgl.ai/wheels-test/repo.html\n\n!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html\n\n!pip install ogb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:09:16.088366Z","iopub.execute_input":"2025-04-10T16:09:16.088688Z","iopub.status.idle":"2025-04-10T16:13:04.703596Z","shell.execute_reply.started":"2025-04-10T16:09:16.088660Z","shell.execute_reply":"2025-04-10T16:13:04.702588Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html\nCollecting dgl\n  Downloading https://data.dgl.ai/wheels/torch-2.4/cu124/dgl-2.4.0%2Bcu124-cp310-cp310-manylinux1_x86_64.whl (347.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.4.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dgl) (24.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dgl) (2.2.3)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.11.0a2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from dgl) (6.0.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.32.3)\nRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.13.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.67.1)\nCollecting torch<=2.4.0 (from dgl)\n  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->dgl) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->dgl) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (3.17.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (1.13.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch<=2.4.0->dgl)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch<=2.4.0->dgl)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch<=2.4.0->dgl)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0->dgl) (12.6.85)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<=2.4.0->dgl) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->dgl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->dgl) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->dgl) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->dgl) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<=2.4.0->dgl) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->dgl) (2024.2.0)\nDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m958.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, dgl\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\ntorchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dgl-2.4.0+cu124 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0\nCollecting ogb\n  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.4.0)\nRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.4)\nRequirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.67.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\nRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.3)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.17.0)\nRequirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.3.0)\nCollecting outdated>=0.2.0 (from ogb)\n  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (2.4.1)\nRequirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (75.1.0)\nCollecting littleutils (from outdated>=0.2.0->ogb)\n  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2025.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.6.85)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.0->ogb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.0->ogb) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.0->ogb) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.0->ogb) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.1.31)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.0->ogb) (2024.2.0)\nDownloading ogb-1.3.6-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\nDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\nInstalling collected packages: littleutils, outdated, ogb\nSuccessfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import time\n\nimport dgl\nimport dgl.nn.pytorch as dglnn\n\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport tqdm\nfrom ogb.nodeproppred import DglNodePropPredDataset\n\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:17:21.848827Z","iopub.execute_input":"2025-04-10T16:17:21.849492Z","iopub.status.idle":"2025-04-10T16:17:21.853963Z","shell.execute_reply.started":"2025-04-10T16:17:21.849458Z","shell.execute_reply":"2025-04-10T16:17:21.853317Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Загрузка датасета","metadata":{}},{"cell_type":"code","source":"device = th.device(\"cuda\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:17:24.488159Z","iopub.execute_input":"2025-04-10T16:17:24.488439Z","iopub.status.idle":"2025-04-10T16:17:24.494241Z","shell.execute_reply.started":"2025-04-10T16:17:24.488416Z","shell.execute_reply":"2025-04-10T16:17:24.493468Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"data = DglNodePropPredDataset(name=\"ogbn-products\")\nsplitted_idx = data.get_idx_split()\ntrain_idx, val_idx, test_idx = (\n    splitted_idx[\"train\"],\n    splitted_idx[\"valid\"],\n    splitted_idx[\"test\"],\n)\ngraph, labels = data[0]\nnfeat = graph.ndata.pop(\"feat\").to(device)\nlabels = labels[:, 0].to(device)\n\nin_feats = nfeat.shape[1]\nn_classes = (labels.max() + 1).item()\n# Create csr/coo/csc formats before launching sampling processes\n# This avoids creating certain formats in each data loader process, which saves momory and CPU.\ngraph.create_formats_()\n# Pack data\ndata = (\n    train_idx,\n    val_idx,\n    test_idx,\n    in_feats,\n    labels,\n    n_classes,\n    nfeat,\n    graph,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:17:28.418430Z","iopub.execute_input":"2025-04-10T16:17:28.418852Z","iopub.status.idle":"2025-04-10T16:19:56.743649Z","shell.execute_reply.started":"2025-04-10T16:17:28.418810Z","shell.execute_reply":"2025-04-10T16:19:56.742855Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"This will download 1.38GB. Will you proceed? (y/N)\n y\n"},{"name":"stdout","text":"Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n","output_type":"stream"},{"name":"stderr","text":"Downloaded 1.38 GB: 100%|██████████| 1414/1414 [00:37<00:00, 38.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/products.zip\nLoading necessary files...\nThis might take a while.\nProcessing graphs...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Converting graphs into DGL objects...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saving...\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 1000\nNUM_WORKERS = 4\nNUM_HIDDEN = 256\nNUM_LAYERS = 3\nDROPOUT = 0.5\nNUM_EPOCHS = 10\nEVAL_EVERY = 1\nLOG_EVERY = 20\nSAVE_PRED = \"\"\nFAN_OUT = \"5,10,15\"\nLR = 0.003\nWD = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:20:45.126390Z","iopub.execute_input":"2025-04-10T16:20:45.126676Z","iopub.status.idle":"2025-04-10T16:20:45.130953Z","shell.execute_reply.started":"2025-04-10T16:20:45.126653Z","shell.execute_reply":"2025-04-10T16:20:45.130285Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"SAGE(\n    in_feats,\n    n_classes,\n    BATCH_SIZE,\n    NUM_WORKERS\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:21:49.473501Z","iopub.execute_input":"2025-04-10T16:21:49.473808Z","iopub.status.idle":"2025-04-10T16:21:49.483732Z","shell.execute_reply.started":"2025-04-10T16:21:49.473785Z","shell.execute_reply":"2025-04-10T16:21:49.483079Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"SAGE(\n  (layers): ModuleList(\n    (0): SAGEConv(\n      (feat_drop): Dropout(p=0.0, inplace=False)\n      (fc_neigh): Linear(in_features=100, out_features=256, bias=False)\n      (fc_self): Linear(in_features=100, out_features=256, bias=True)\n    )\n    (1): SAGEConv(\n      (feat_drop): Dropout(p=0.0, inplace=False)\n      (fc_neigh): Linear(in_features=256, out_features=256, bias=False)\n      (fc_self): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (2): SAGEConv(\n      (feat_drop): Dropout(p=0.0, inplace=False)\n      (fc_neigh): Linear(in_features=256, out_features=47, bias=False)\n      (fc_self): Linear(in_features=256, out_features=47, bias=True)\n    )\n  )\n  (dropout): Dropout(p=0.5, inplace=False)\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"class SAGE(nn.Module):\n    def __init__(\n        self, in_feats, n_classes, batch_size, num_workers\n    ):\n        super().__init__()\n        self.n_layers = 3\n        self.n_hidden = 256\n        self.n_classes = n_classes\n        self.layers = nn.ModuleList()\n        self.layers.append(dglnn.SAGEConv(in_feats, self.n_hidden, \"mean\"))\n        for i in range(1, self.n_layers - 1):\n            self.layers.append(dglnn.SAGEConv(self.n_hidden, self.n_hidden, \"mean\"))\n        self.layers.append(dglnn.SAGEConv(self.n_hidden, n_classes, \"mean\"))\n        self.dropout = nn.Dropout(0.5)\n        self.activation = F.relu\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n    def forward(self, blocks, x):\n        h = x\n        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n            # We need to first copy the representation of nodes on the RHS from the\n            # appropriate nodes on the LHS.\n            # Note that the shape of h is (num_nodes_LHS, D) and the shape of h_dst\n            # would be (num_nodes_RHS, D)\n            h_dst = h[: block.num_dst_nodes()]\n            # Then we compute the updated representation on the RHS.\n            # The shape of h now becomes (num_nodes_RHS, D)\n            h = layer(block, (h, h_dst))\n            if l != len(self.layers) - 1:\n                h = self.activation(h)\n                h = self.dropout(h)\n        return h\n\n    def inference(self, g, x, device):\n        \"\"\"\n        Inference with the GraphSAGE model on full neighbors (i.e. without neighbor sampling).\n        g : the entire graph.\n        x : the input of entire node set.\n        The inference code is written in a fashion that it could handle any number of nodes and\n        layers.\n        \"\"\"\n        # During inference with sampling, multi-layer blocks are very inefficient because\n        # lots of computations in the first few layers are repeated.\n        # Therefore, we compute the representation of all nodes layer by layer.  The nodes\n        # on each layer are of course splitted in batches.\n        # TODO: can we standardize this?\n        for l, layer in enumerate(self.layers):\n            y = th.zeros(\n                g.num_nodes(),\n                self.n_hidden if l != len(self.layers) - 1 else self.n_classes,\n            ).to(device)\n\n            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n            dataloader = dgl.dataloading.DataLoader(\n                g,\n                th.arange(g.num_nodes()),\n                sampler,\n                batch_size=self.batch_size,\n                shuffle=True,\n                drop_last=False,\n                num_workers=self.num_workers,\n            )\n\n            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n                block = blocks[0].int().to(device)\n\n                h = x[input_nodes]\n                h_dst = h[: block.num_dst_nodes()]\n                h = layer(block, (h, h_dst))\n                if l != len(self.layers) - 1:\n                    h = self.activation(h)\n                    h = self.dropout(h)\n\n                y[output_nodes] = h\n\n            x = y\n        return y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:19:56.744784Z","iopub.execute_input":"2025-04-10T16:19:56.745113Z","iopub.status.idle":"2025-04-10T16:19:56.754481Z","shell.execute_reply.started":"2025-04-10T16:19:56.745088Z","shell.execute_reply":"2025-04-10T16:19:56.753560Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def compute_acc(pred, labels):\n    \"\"\"\n    Compute the accuracy of prediction given the labels.\n    \"\"\"\n    return (th.argmax(pred, dim=1) == labels).float().sum() / len(pred)\n\n\ndef evaluate(model, g, nfeat, labels, val_nid, test_nid, device):\n    \"\"\"\n    Evaluate the model on the validation set specified by ``val_mask``.\n    g : The entire graph.\n    inputs : The features of all the nodes.\n    labels : The labels of all the nodes.\n    val_mask : A 0-1 mask indicating which nodes do we actually compute the accuracy for.\n    device : The GPU device to evaluate on.\n    \"\"\"\n    model.eval()\n    with th.no_grad():\n        pred = model.inference(g, nfeat, device)\n    model.train()\n    return (\n        compute_acc(pred[val_nid], labels[val_nid]),\n        compute_acc(pred[test_nid], labels[test_nid]),\n        pred,\n    )\n\ndef load_subtensor(nfeat, labels, seeds, input_nodes):\n    \"\"\"\n    Extracts features and labels for a set of nodes.\n    \"\"\"\n    batch_inputs = nfeat[input_nodes]\n    batch_labels = labels[seeds]\n    return batch_inputs, batch_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:19:56.755838Z","iopub.execute_input":"2025-04-10T16:19:56.756094Z","iopub.status.idle":"2025-04-10T16:19:56.778999Z","shell.execute_reply.started":"2025-04-10T16:19:56.756074Z","shell.execute_reply":"2025-04-10T16:19:56.778224Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"train_nid, val_nid, test_nid, in_feats, labels, n_classes, nfeat, g = data\n\n# Create PyTorch DataLoader for constructing blocks\nsampler = dgl.dataloading.MultiLayerNeighborSampler(\n    [int(fanout) for fanout in FAN_OUT.split(\",\")]\n)\ndataloader = dgl.dataloading.DataLoader(\n    g,\n    train_nid,\n    sampler,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    drop_last=False,\n    num_workers=NUM_WORKERS,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:20:51.467473Z","iopub.execute_input":"2025-04-10T16:20:51.467750Z","iopub.status.idle":"2025-04-10T16:20:51.473435Z","shell.execute_reply.started":"2025-04-10T16:20:51.467727Z","shell.execute_reply":"2025-04-10T16:20:51.472532Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"model = SAGE(\n    in_feats,\n    n_classes,  # 47\n    BATCH_SIZE,\n    NUM_WORKERS\n)\nmodel = model.to(device)\nloss_fcn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:22:29.951998Z","iopub.execute_input":"2025-04-10T16:22:29.952290Z","iopub.status.idle":"2025-04-10T16:22:31.579245Z","shell.execute_reply.started":"2025-04-10T16:22:29.952266Z","shell.execute_reply":"2025-04-10T16:22:31.578553Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"avg = 0\niter_tput = []\nbest_eval_acc = 0\nbest_test_acc = 0\nfor epoch in range(NUM_EPOCHS):\n    tic = time.time()\n\n    # Loop over the dataloader to sample the computation dependency graph as a list of\n    # blocks.\n    for step, (input_nodes, seeds, blocks) in enumerate(dataloader):\n        tic_step = time.time()\n\n        # copy block to gpu\n        blocks = [blk.int().to(device) for blk in blocks]\n\n        # Load the input features as well as output labels\n        batch_inputs, batch_labels = load_subtensor(\n            nfeat, labels, seeds, input_nodes\n        )\n\n        # Compute loss and prediction\n        batch_pred = model(blocks, batch_inputs)\n        loss = loss_fcn(batch_pred, batch_labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        iter_tput.append(len(seeds) / (time.time() - tic_step))\n        if step % LOG_EVERY == 0:\n            acc = compute_acc(batch_pred, batch_labels)\n            gpu_mem_alloc = (\n                th.cuda.max_memory_allocated() / 1000000\n                if th.cuda.is_available()\n                else 0\n            )\n            print(\n                \"Epoch {:05d} | Step {:05d} | Loss {:.4f} | Train Acc {:.4f} | Speed (samples/sec) {:.4f} | GPU {:.1f} MB\".format(\n                    epoch,\n                    step,\n                    loss.item(),\n                    acc.item(),\n                    np.mean(iter_tput[3:]),\n                    gpu_mem_alloc,\n                )\n            )\n        gc.collect()\n\n    toc = time.time()\n    print(\"Epoch Time(s): {:.4f}\".format(toc - tic))\n    if epoch >= 5:\n        avg += toc - tic\n    if epoch % EVAL_EVERY == 0 and epoch != 0:\n        eval_acc, test_acc, pred = evaluate(\n            model, g, nfeat, labels, val_nid, test_nid, device\n        )\n        if SAVE_PRED:\n            np.savetxt(\n                SAVE_PRED + \"%02d\" % epoch,\n                pred.argmax(1).cpu().numpy(),\n                \"%d\",\n            )\n        print(\"Eval Acc {:.4f}\".format(eval_acc))\n        if eval_acc > best_eval_acc:\n            best_eval_acc = eval_acc\n            best_test_acc = test_acc\n        print(\n            \"Best Eval Acc {:.4f} Test Acc {:.4f}\".format(\n                best_eval_acc, best_test_acc\n            )\n        )\n    gc.collect()\n\nprint(\"Avg epoch time: {}\".format(avg / (epoch - 4)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:22:34.383690Z","iopub.execute_input":"2025-04-10T16:22:34.384325Z","iopub.status.idle":"2025-04-10T16:37:27.832580Z","shell.execute_reply.started":"2025-04-10T16:22:34.384292Z","shell.execute_reply":"2025-04-10T16:37:27.831440Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/dgl/dataloading/dataloader.py:1144: DGLWarning: Dataloader CPU affinity opt is not enabled, consider switching it on (see enable_cpu_affinity() or CPU best practices for DGL [https://docs.dgl.ai/tutorials/cpu/cpu_best_practises.html])\n  dgl_warning(\n/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 00000 | Step 00000 | Loss 7.8669 | Train Acc 0.0160 | Speed (samples/sec) nan | GPU 1697.9 MB\nEpoch 00000 | Step 00020 | Loss 1.2444 | Train Acc 0.6660 | Speed (samples/sec) 54648.9076 | GPU 1715.1 MB\nEpoch 00000 | Step 00040 | Loss 0.8931 | Train Acc 0.7840 | Speed (samples/sec) 56410.1906 | GPU 1716.2 MB\nEpoch 00000 | Step 00060 | Loss 0.7186 | Train Acc 0.8090 | Speed (samples/sec) 56845.8301 | GPU 1716.2 MB\nEpoch 00000 | Step 00080 | Loss 0.7525 | Train Acc 0.8160 | Speed (samples/sec) 57523.4064 | GPU 1716.2 MB\nEpoch 00000 | Step 00100 | Loss 0.6755 | Train Acc 0.8390 | Speed (samples/sec) 58215.0048 | GPU 1716.2 MB\nEpoch 00000 | Step 00120 | Loss 0.4988 | Train Acc 0.8620 | Speed (samples/sec) 58889.3530 | GPU 1716.2 MB\nEpoch 00000 | Step 00140 | Loss 0.5575 | Train Acc 0.8520 | Speed (samples/sec) 58809.2217 | GPU 1716.2 MB\nEpoch 00000 | Step 00160 | Loss 0.5117 | Train Acc 0.8770 | Speed (samples/sec) 58599.1781 | GPU 1716.2 MB\nEpoch 00000 | Step 00180 | Loss 0.4651 | Train Acc 0.8830 | Speed (samples/sec) 58789.8619 | GPU 1716.7 MB\nEpoch Time(s): 38.7059\nEpoch 00001 | Step 00000 | Loss 0.4586 | Train Acc 0.8760 | Speed (samples/sec) 58798.6743 | GPU 1716.7 MB\nEpoch 00001 | Step 00020 | Loss 0.4364 | Train Acc 0.8790 | Speed (samples/sec) 58483.2977 | GPU 1716.7 MB\nEpoch 00001 | Step 00040 | Loss 0.4763 | Train Acc 0.8640 | Speed (samples/sec) 58742.1297 | GPU 1716.7 MB\nEpoch 00001 | Step 00060 | Loss 0.4392 | Train Acc 0.8750 | Speed (samples/sec) 58459.9079 | GPU 1716.7 MB\nEpoch 00001 | Step 00080 | Loss 0.4883 | Train Acc 0.8730 | Speed (samples/sec) 57963.9533 | GPU 1716.7 MB\nEpoch 00001 | Step 00100 | Loss 0.4694 | Train Acc 0.8730 | Speed (samples/sec) 57821.4518 | GPU 1716.7 MB\nEpoch 00001 | Step 00120 | Loss 0.4299 | Train Acc 0.8920 | Speed (samples/sec) 57926.5082 | GPU 1716.7 MB\nEpoch 00001 | Step 00140 | Loss 0.4100 | Train Acc 0.8830 | Speed (samples/sec) 57884.1819 | GPU 1716.7 MB\nEpoch 00001 | Step 00160 | Loss 0.3672 | Train Acc 0.9010 | Speed (samples/sec) 57823.2876 | GPU 1716.7 MB\nEpoch 00001 | Step 00180 | Loss 0.3513 | Train Acc 0.9030 | Speed (samples/sec) 57606.4811 | GPU 1716.7 MB\nEpoch Time(s): 39.8620\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:16<00:00, 149.27it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 147.78it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 139.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.8967\nBest Eval Acc 0.8967 Test Acc 0.7446\nEpoch 00002 | Step 00000 | Loss 0.4104 | Train Acc 0.8770 | Speed (samples/sec) 57623.3602 | GPU 6350.8 MB\nEpoch 00002 | Step 00020 | Loss 0.3954 | Train Acc 0.9030 | Speed (samples/sec) 57510.4399 | GPU 6350.8 MB\nEpoch 00002 | Step 00040 | Loss 0.3700 | Train Acc 0.9010 | Speed (samples/sec) 57372.3442 | GPU 6350.8 MB\nEpoch 00002 | Step 00060 | Loss 0.3717 | Train Acc 0.9060 | Speed (samples/sec) 57373.6411 | GPU 6350.8 MB\nEpoch 00002 | Step 00080 | Loss 0.3743 | Train Acc 0.8940 | Speed (samples/sec) 57276.2813 | GPU 6350.8 MB\nEpoch 00002 | Step 00100 | Loss 0.4032 | Train Acc 0.8900 | Speed (samples/sec) 57341.6840 | GPU 6350.8 MB\nEpoch 00002 | Step 00120 | Loss 0.4065 | Train Acc 0.8940 | Speed (samples/sec) 57350.1721 | GPU 6350.8 MB\nEpoch 00002 | Step 00140 | Loss 0.3312 | Train Acc 0.9050 | Speed (samples/sec) 57233.8404 | GPU 6350.8 MB\nEpoch 00002 | Step 00160 | Loss 0.3121 | Train Acc 0.9080 | Speed (samples/sec) 57006.3270 | GPU 6350.8 MB\nEpoch 00002 | Step 00180 | Loss 0.4247 | Train Acc 0.8820 | Speed (samples/sec) 56997.6020 | GPU 6350.8 MB\nEpoch Time(s): 40.3050\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:16<00:00, 148.31it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 149.78it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 139.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9043\nBest Eval Acc 0.9043 Test Acc 0.7613\nEpoch 00003 | Step 00000 | Loss 0.3738 | Train Acc 0.8910 | Speed (samples/sec) 56990.7477 | GPU 6808.4 MB\nEpoch 00003 | Step 00020 | Loss 0.3546 | Train Acc 0.8980 | Speed (samples/sec) 56915.2151 | GPU 6808.4 MB\nEpoch 00003 | Step 00040 | Loss 0.3161 | Train Acc 0.9150 | Speed (samples/sec) 56861.2988 | GPU 6808.4 MB\nEpoch 00003 | Step 00060 | Loss 0.3659 | Train Acc 0.8900 | Speed (samples/sec) 56762.7943 | GPU 6808.4 MB\nEpoch 00003 | Step 00080 | Loss 0.3483 | Train Acc 0.9050 | Speed (samples/sec) 56901.0358 | GPU 6808.4 MB\nEpoch 00003 | Step 00100 | Loss 0.3085 | Train Acc 0.9140 | Speed (samples/sec) 56866.0150 | GPU 6808.4 MB\nEpoch 00003 | Step 00120 | Loss 0.4290 | Train Acc 0.8810 | Speed (samples/sec) 56809.3350 | GPU 6808.4 MB\nEpoch 00003 | Step 00140 | Loss 0.3852 | Train Acc 0.8930 | Speed (samples/sec) 56760.0728 | GPU 6808.4 MB\nEpoch 00003 | Step 00160 | Loss 0.4164 | Train Acc 0.8910 | Speed (samples/sec) 56598.7955 | GPU 6808.4 MB\nEpoch 00003 | Step 00180 | Loss 0.3779 | Train Acc 0.9010 | Speed (samples/sec) 56599.4167 | GPU 6808.4 MB\nEpoch Time(s): 40.3828\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:16<00:00, 147.03it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 147.92it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 138.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9071\nBest Eval Acc 0.9071 Test Acc 0.7591\nEpoch 00004 | Step 00000 | Loss 0.3537 | Train Acc 0.8960 | Speed (samples/sec) 56593.3866 | GPU 6808.4 MB\nEpoch 00004 | Step 00020 | Loss 0.3233 | Train Acc 0.9050 | Speed (samples/sec) 56453.3500 | GPU 6808.4 MB\nEpoch 00004 | Step 00040 | Loss 0.4071 | Train Acc 0.8840 | Speed (samples/sec) 56275.3187 | GPU 6808.4 MB\nEpoch 00004 | Step 00060 | Loss 0.3386 | Train Acc 0.9020 | Speed (samples/sec) 56249.2954 | GPU 6808.4 MB\nEpoch 00004 | Step 00080 | Loss 0.3526 | Train Acc 0.9060 | Speed (samples/sec) 56240.3154 | GPU 6808.4 MB\nEpoch 00004 | Step 00100 | Loss 0.3090 | Train Acc 0.9160 | Speed (samples/sec) 56137.6902 | GPU 6808.4 MB\nEpoch 00004 | Step 00120 | Loss 0.3349 | Train Acc 0.9080 | Speed (samples/sec) 56076.4419 | GPU 6808.4 MB\nEpoch 00004 | Step 00140 | Loss 0.3689 | Train Acc 0.8940 | Speed (samples/sec) 55941.5762 | GPU 6808.4 MB\nEpoch 00004 | Step 00160 | Loss 0.3551 | Train Acc 0.8940 | Speed (samples/sec) 55879.0833 | GPU 6808.4 MB\nEpoch 00004 | Step 00180 | Loss 0.3264 | Train Acc 0.9160 | Speed (samples/sec) 55716.0804 | GPU 6808.4 MB\nEpoch Time(s): 40.4742\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:16<00:00, 149.02it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 148.47it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 139.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9094\nBest Eval Acc 0.9094 Test Acc 0.7772\nEpoch 00005 | Step 00000 | Loss 0.2947 | Train Acc 0.9130 | Speed (samples/sec) 55623.1758 | GPU 6810.6 MB\nEpoch 00005 | Step 00020 | Loss 0.3446 | Train Acc 0.9100 | Speed (samples/sec) 55582.7376 | GPU 6810.6 MB\nEpoch 00005 | Step 00040 | Loss 0.3541 | Train Acc 0.9100 | Speed (samples/sec) 55488.3359 | GPU 6810.6 MB\nEpoch 00005 | Step 00060 | Loss 0.3388 | Train Acc 0.9100 | Speed (samples/sec) 55400.6503 | GPU 6810.6 MB\nEpoch 00005 | Step 00080 | Loss 0.3197 | Train Acc 0.9100 | Speed (samples/sec) 55283.8135 | GPU 6810.6 MB\nEpoch 00005 | Step 00100 | Loss 0.3317 | Train Acc 0.9040 | Speed (samples/sec) 55252.6528 | GPU 6810.6 MB\nEpoch 00005 | Step 00120 | Loss 0.3654 | Train Acc 0.9000 | Speed (samples/sec) 55171.0194 | GPU 6810.6 MB\nEpoch 00005 | Step 00140 | Loss 0.2664 | Train Acc 0.9330 | Speed (samples/sec) 55062.2229 | GPU 6810.6 MB\nEpoch 00005 | Step 00160 | Loss 0.3285 | Train Acc 0.9100 | Speed (samples/sec) 55054.3122 | GPU 6810.6 MB\nEpoch 00005 | Step 00180 | Loss 0.3651 | Train Acc 0.8830 | Speed (samples/sec) 55044.9484 | GPU 6810.6 MB\nEpoch Time(s): 40.7336\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:16<00:00, 151.07it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 148.88it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 138.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9113\nBest Eval Acc 0.9113 Test Acc 0.7722\nEpoch 00006 | Step 00000 | Loss 0.3363 | Train Acc 0.9070 | Speed (samples/sec) 55014.7697 | GPU 6813.5 MB\nEpoch 00006 | Step 00020 | Loss 0.2672 | Train Acc 0.9330 | Speed (samples/sec) 54938.0231 | GPU 6813.5 MB\nEpoch 00006 | Step 00040 | Loss 0.3681 | Train Acc 0.9040 | Speed (samples/sec) 54882.8142 | GPU 6813.5 MB\nEpoch 00006 | Step 00060 | Loss 0.3664 | Train Acc 0.9000 | Speed (samples/sec) 54939.1019 | GPU 6813.5 MB\nEpoch 00006 | Step 00080 | Loss 0.3550 | Train Acc 0.9060 | Speed (samples/sec) 54959.0512 | GPU 6813.5 MB\nEpoch 00006 | Step 00100 | Loss 0.2943 | Train Acc 0.9110 | Speed (samples/sec) 54965.5408 | GPU 6813.5 MB\nEpoch 00006 | Step 00120 | Loss 0.3489 | Train Acc 0.8960 | Speed (samples/sec) 54943.7449 | GPU 6813.5 MB\nEpoch 00006 | Step 00140 | Loss 0.3996 | Train Acc 0.8740 | Speed (samples/sec) 54831.6156 | GPU 6813.5 MB\nEpoch 00006 | Step 00160 | Loss 0.3521 | Train Acc 0.9040 | Speed (samples/sec) 54783.4810 | GPU 6813.5 MB\nEpoch 00006 | Step 00180 | Loss 0.3183 | Train Acc 0.9140 | Speed (samples/sec) 54713.2109 | GPU 6813.5 MB\nEpoch Time(s): 40.2654\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:16<00:00, 150.82it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 146.67it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 139.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9151\nBest Eval Acc 0.9151 Test Acc 0.7803\nEpoch 00007 | Step 00000 | Loss 0.2741 | Train Acc 0.9180 | Speed (samples/sec) 54669.2209 | GPU 6813.5 MB\nEpoch 00007 | Step 00020 | Loss 0.3218 | Train Acc 0.9050 | Speed (samples/sec) 54565.4360 | GPU 6813.5 MB\nEpoch 00007 | Step 00040 | Loss 0.3433 | Train Acc 0.9120 | Speed (samples/sec) 54544.3603 | GPU 6813.5 MB\nEpoch 00007 | Step 00060 | Loss 0.2934 | Train Acc 0.9230 | Speed (samples/sec) 54507.5004 | GPU 6813.5 MB\nEpoch 00007 | Step 00080 | Loss 0.3237 | Train Acc 0.9210 | Speed (samples/sec) 54427.3952 | GPU 6813.5 MB\nEpoch 00007 | Step 00100 | Loss 0.3413 | Train Acc 0.9030 | Speed (samples/sec) 54359.1880 | GPU 6813.5 MB\nEpoch 00007 | Step 00120 | Loss 0.3083 | Train Acc 0.9160 | Speed (samples/sec) 54392.6064 | GPU 6813.5 MB\nEpoch 00007 | Step 00140 | Loss 0.3068 | Train Acc 0.9150 | Speed (samples/sec) 54291.0971 | GPU 6813.5 MB\nEpoch 00007 | Step 00160 | Loss 0.3340 | Train Acc 0.9100 | Speed (samples/sec) 54282.3166 | GPU 6813.5 MB\nEpoch 00007 | Step 00180 | Loss 0.3070 | Train Acc 0.9150 | Speed (samples/sec) 54320.9258 | GPU 6813.5 MB\nEpoch Time(s): 40.3664\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:16<00:00, 151.63it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 147.57it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 139.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9143\nBest Eval Acc 0.9151 Test Acc 0.7803\nEpoch 00008 | Step 00000 | Loss 0.3176 | Train Acc 0.9060 | Speed (samples/sec) 54266.8503 | GPU 6813.5 MB\nEpoch 00008 | Step 00020 | Loss 0.2783 | Train Acc 0.9210 | Speed (samples/sec) 54247.5224 | GPU 6813.5 MB\nEpoch 00008 | Step 00040 | Loss 0.2798 | Train Acc 0.9240 | Speed (samples/sec) 54261.7108 | GPU 6813.5 MB\nEpoch 00008 | Step 00060 | Loss 0.3063 | Train Acc 0.9150 | Speed (samples/sec) 54244.6313 | GPU 6813.5 MB\nEpoch 00008 | Step 00080 | Loss 0.2800 | Train Acc 0.9220 | Speed (samples/sec) 54227.9949 | GPU 6813.5 MB\nEpoch 00008 | Step 00100 | Loss 0.3361 | Train Acc 0.9020 | Speed (samples/sec) 54213.2014 | GPU 6813.5 MB\nEpoch 00008 | Step 00120 | Loss 0.3206 | Train Acc 0.9040 | Speed (samples/sec) 54193.4070 | GPU 6813.5 MB\nEpoch 00008 | Step 00140 | Loss 0.2509 | Train Acc 0.9350 | Speed (samples/sec) 54138.6640 | GPU 6813.5 MB\nEpoch 00008 | Step 00160 | Loss 0.2907 | Train Acc 0.9110 | Speed (samples/sec) 54092.3547 | GPU 6813.5 MB\nEpoch 00008 | Step 00180 | Loss 0.2862 | Train Acc 0.9240 | Speed (samples/sec) 54040.5136 | GPU 6813.5 MB\nEpoch Time(s): 40.3849\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:16<00:00, 150.95it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 147.38it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 139.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9173\nBest Eval Acc 0.9173 Test Acc 0.7779\nEpoch 00009 | Step 00000 | Loss 0.2672 | Train Acc 0.9200 | Speed (samples/sec) 54046.1353 | GPU 6813.5 MB\nEpoch 00009 | Step 00020 | Loss 0.3120 | Train Acc 0.9190 | Speed (samples/sec) 54010.8009 | GPU 6813.5 MB\nEpoch 00009 | Step 00040 | Loss 0.3155 | Train Acc 0.9150 | Speed (samples/sec) 53939.0448 | GPU 6813.5 MB\nEpoch 00009 | Step 00060 | Loss 0.2699 | Train Acc 0.9190 | Speed (samples/sec) 53885.2760 | GPU 6813.5 MB\nEpoch 00009 | Step 00080 | Loss 0.3167 | Train Acc 0.9030 | Speed (samples/sec) 53821.5605 | GPU 6813.5 MB\nEpoch 00009 | Step 00100 | Loss 0.2884 | Train Acc 0.9210 | Speed (samples/sec) 53769.6091 | GPU 6813.5 MB\nEpoch 00009 | Step 00120 | Loss 0.3383 | Train Acc 0.9120 | Speed (samples/sec) 53727.2403 | GPU 6813.5 MB\nEpoch 00009 | Step 00140 | Loss 0.2700 | Train Acc 0.9250 | Speed (samples/sec) 53657.2414 | GPU 6813.5 MB\nEpoch 00009 | Step 00160 | Loss 0.2587 | Train Acc 0.9230 | Speed (samples/sec) 53615.0276 | GPU 6813.5 MB\nEpoch 00009 | Step 00180 | Loss 0.2785 | Train Acc 0.9150 | Speed (samples/sec) 53582.1822 | GPU 6813.5 MB\nEpoch Time(s): 40.8069\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:16<00:00, 150.86it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 145.98it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 138.66it/s]","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9159\nBest Eval Acc 0.9173 Test Acc 0.7779\nAvg epoch time: 40.5114604473114\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\ntorch.save(model, './SAGE.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T16:37:27.833993Z","iopub.execute_input":"2025-04-10T16:37:27.834336Z","iopub.status.idle":"2025-04-10T16:37:27.843140Z","shell.execute_reply.started":"2025-04-10T16:37:27.834299Z","shell.execute_reply":"2025-04-10T16:37:27.842322Z"}},"outputs":[],"execution_count":17}]}