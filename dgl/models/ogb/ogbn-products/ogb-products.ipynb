{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install  dgl==2.0.0 -f https://data.dgl.ai/wheels/repo.html\n# !pip install  dglgo==0.0.2 -f https://data.dgl.ai/wheels-test/repo.html\n\n!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.4/cu121/repo.html\n\n!pip install ogb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:54:56.166124Z","iopub.execute_input":"2025-04-03T17:54:56.166484Z","iopub.status.idle":"2025-04-03T17:57:58.637417Z","shell.execute_reply.started":"2025-04-03T17:54:56.166460Z","shell.execute_reply":"2025-04-03T17:57:58.636275Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://data.dgl.ai/wheels/torch-2.4/cu121/repo.html\nCollecting dgl\n  Downloading https://data.dgl.ai/wheels/torch-2.4/cu121/dgl-2.4.0%2Bcu121-cp310-cp310-manylinux1_x86_64.whl (355.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.2/355.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.4.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dgl) (24.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dgl) (2.2.3)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.11.0a2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from dgl) (6.0.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.32.3)\nRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.13.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.67.1)\nCollecting torch<=2.4.0 (from dgl)\n  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->dgl) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->dgl) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->dgl) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (3.17.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (1.13.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch<=2.4.0->dgl)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<=2.4.0->dgl)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch<=2.4.0->dgl)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch<=2.4.0->dgl)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch<=2.4.0->dgl)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0->dgl) (12.6.85)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<=2.4.0->dgl) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->dgl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->dgl) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->dgl) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->dgl) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<=2.4.0->dgl) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->dgl) (2024.2.0)\nDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, dgl\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\ntorchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dgl-2.4.0+cu121 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0\nCollecting ogb\n  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.4.0)\nRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.4)\nRequirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.67.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\nRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.3)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.17.0)\nRequirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.3.0)\nCollecting outdated>=0.2.0 (from ogb)\n  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb) (2.4.1)\nRequirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (75.1.0)\nCollecting littleutils (from outdated>=0.2.0->ogb)\n  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2025.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.6.85)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.0->ogb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.0->ogb) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.0->ogb) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.0->ogb) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.1.31)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.0->ogb) (2024.2.0)\nDownloading ogb-1.3.6-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\nDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\nInstalling collected packages: littleutils, outdated, ogb\nSuccessfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import time\n\nimport dgl\nimport dgl.nn.pytorch as dglnn\n\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport tqdm\nfrom ogb.nodeproppred import DglNodePropPredDataset\n\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:57:58.639967Z","iopub.execute_input":"2025-04-03T17:57:58.640206Z","iopub.status.idle":"2025-04-03T17:58:02.598825Z","shell.execute_reply.started":"2025-04-03T17:57:58.640185Z","shell.execute_reply":"2025-04-03T17:58:02.598084Z"}},"outputs":[{"name":"stdout","text":"Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n","output_type":"stream"},{"name":"stderr","text":"DGL backend not selected or invalid.  Assuming PyTorch for now.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Загрузка датасета","metadata":{}},{"cell_type":"code","source":"device = th.device(\"cuda\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:58:02.599944Z","iopub.execute_input":"2025-04-03T17:58:02.600464Z","iopub.status.idle":"2025-04-03T17:58:02.606202Z","shell.execute_reply.started":"2025-04-03T17:58:02.600437Z","shell.execute_reply":"2025-04-03T17:58:02.605180Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"data = DglNodePropPredDataset(name=\"ogbn-products\")\nsplitted_idx = data.get_idx_split()\ntrain_idx, val_idx, test_idx = (\n    splitted_idx[\"train\"],\n    splitted_idx[\"valid\"],\n    splitted_idx[\"test\"],\n)\ngraph, labels = data[0]\nnfeat = graph.ndata.pop(\"feat\").to(device)\nlabels = labels[:, 0].to(device)\n\nin_feats = nfeat.shape[1]\nn_classes = (labels.max() + 1).item()\n# Create csr/coo/csc formats before launching sampling processes\n# This avoids creating certain formats in each data loader process, which saves momory and CPU.\ngraph.create_formats_()\n# Pack data\ndata = (\n    train_idx,\n    val_idx,\n    test_idx,\n    in_feats,\n    labels,\n    n_classes,\n    nfeat,\n    graph,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:58:02.607214Z","iopub.execute_input":"2025-04-03T17:58:02.607504Z","iopub.status.idle":"2025-04-03T18:09:40.410390Z","shell.execute_reply.started":"2025-04-03T17:58:02.607475Z","shell.execute_reply":"2025-04-03T18:09:40.409586Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"This will download 1.38GB. Will you proceed? (y/N)\n y\n"},{"name":"stdout","text":"Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n","output_type":"stream"},{"name":"stderr","text":"Downloaded 1.38 GB: 100%|██████████| 1414/1414 [00:40<00:00, 34.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/products.zip\nLoading necessary files...\nThis might take a while.\nProcessing graphs...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Converting graphs into DGL objects...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saving...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 1000\nNUM_WORKERS = 4\nNUM_HIDDEN = 256\nNUM_LAYERS = 3\nDROPOUT = 0.5\nNUM_EPOCHS = 10\nEVAL_EVERY = 1\nLOG_EVERY = 20\nSAVE_PRED = \"\"\nFAN_OUT = \"5,10,15\"\nLR = 0.003\nWD = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T18:09:40.411282Z","iopub.execute_input":"2025-04-03T18:09:40.411564Z","iopub.status.idle":"2025-04-03T18:09:40.416487Z","shell.execute_reply.started":"2025-04-03T18:09:40.411534Z","shell.execute_reply":"2025-04-03T18:09:40.415534Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class SAGE(nn.Module):\n    def __init__(\n        self, in_feats, n_hidden, n_classes, n_layers, activation, dropout\n    ):\n        super().__init__()\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.n_classes = n_classes\n        self.layers = nn.ModuleList()\n        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, \"mean\"))\n        for i in range(1, n_layers - 1):\n            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, \"mean\"))\n        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, \"mean\"))\n        self.dropout = nn.Dropout(dropout)\n        self.activation = activation\n\n    def forward(self, blocks, x):\n        h = x\n        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n            # We need to first copy the representation of nodes on the RHS from the\n            # appropriate nodes on the LHS.\n            # Note that the shape of h is (num_nodes_LHS, D) and the shape of h_dst\n            # would be (num_nodes_RHS, D)\n            h_dst = h[: block.num_dst_nodes()]\n            # Then we compute the updated representation on the RHS.\n            # The shape of h now becomes (num_nodes_RHS, D)\n            h = layer(block, (h, h_dst))\n            if l != len(self.layers) - 1:\n                h = self.activation(h)\n                h = self.dropout(h)\n        return h\n\n    def inference(self, g, x, device):\n        \"\"\"\n        Inference with the GraphSAGE model on full neighbors (i.e. without neighbor sampling).\n        g : the entire graph.\n        x : the input of entire node set.\n        The inference code is written in a fashion that it could handle any number of nodes and\n        layers.\n        \"\"\"\n        # During inference with sampling, multi-layer blocks are very inefficient because\n        # lots of computations in the first few layers are repeated.\n        # Therefore, we compute the representation of all nodes layer by layer.  The nodes\n        # on each layer are of course splitted in batches.\n        # TODO: can we standardize this?\n        for l, layer in enumerate(self.layers):\n            y = th.zeros(\n                g.num_nodes(),\n                self.n_hidden if l != len(self.layers) - 1 else self.n_classes,\n            ).to(device)\n\n            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n            dataloader = dgl.dataloading.DataLoader(\n                g,\n                th.arange(g.num_nodes()),\n                sampler,\n                batch_size=BATCH_SIZE,\n                shuffle=True,\n                drop_last=False,\n                num_workers=NUM_WORKERS,\n            )\n\n            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n                block = blocks[0].int().to(device)\n\n                h = x[input_nodes]\n                h_dst = h[: block.num_dst_nodes()]\n                h = layer(block, (h, h_dst))\n                if l != len(self.layers) - 1:\n                    h = self.activation(h)\n                    h = self.dropout(h)\n\n                y[output_nodes] = h\n\n            x = y\n        return y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T18:09:40.417302Z","iopub.execute_input":"2025-04-03T18:09:40.417553Z","iopub.status.idle":"2025-04-03T18:09:40.431975Z","shell.execute_reply.started":"2025-04-03T18:09:40.417531Z","shell.execute_reply":"2025-04-03T18:09:40.431107Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def compute_acc(pred, labels):\n    \"\"\"\n    Compute the accuracy of prediction given the labels.\n    \"\"\"\n    return (th.argmax(pred, dim=1) == labels).float().sum() / len(pred)\n\n\ndef evaluate(model, g, nfeat, labels, val_nid, test_nid, device):\n    \"\"\"\n    Evaluate the model on the validation set specified by ``val_mask``.\n    g : The entire graph.\n    inputs : The features of all the nodes.\n    labels : The labels of all the nodes.\n    val_mask : A 0-1 mask indicating which nodes do we actually compute the accuracy for.\n    device : The GPU device to evaluate on.\n    \"\"\"\n    model.eval()\n    with th.no_grad():\n        pred = model.inference(g, nfeat, device)\n    model.train()\n    return (\n        compute_acc(pred[val_nid], labels[val_nid]),\n        compute_acc(pred[test_nid], labels[test_nid]),\n        pred,\n    )\n\ndef load_subtensor(nfeat, labels, seeds, input_nodes):\n    \"\"\"\n    Extracts features and labels for a set of nodes.\n    \"\"\"\n    batch_inputs = nfeat[input_nodes]\n    batch_labels = labels[seeds]\n    return batch_inputs, batch_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T18:09:40.432851Z","iopub.execute_input":"2025-04-03T18:09:40.433170Z","iopub.status.idle":"2025-04-03T18:09:40.453863Z","shell.execute_reply.started":"2025-04-03T18:09:40.433135Z","shell.execute_reply":"2025-04-03T18:09:40.452903Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"train_nid, val_nid, test_nid, in_feats, labels, n_classes, nfeat, g = data\n\n# Create PyTorch DataLoader for constructing blocks\nsampler = dgl.dataloading.MultiLayerNeighborSampler(\n    [int(fanout) for fanout in FAN_OUT.split(\",\")]\n)\ndataloader = dgl.dataloading.DataLoader(\n    g,\n    train_nid,\n    sampler,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    drop_last=False,\n    num_workers=NUM_WORKERS,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T18:09:40.456405Z","iopub.execute_input":"2025-04-03T18:09:40.456648Z","iopub.status.idle":"2025-04-03T18:09:40.471637Z","shell.execute_reply.started":"2025-04-03T18:09:40.456617Z","shell.execute_reply":"2025-04-03T18:09:40.470908Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model = SAGE(\n    in_feats,\n    NUM_HIDDEN,\n    n_classes,\n    NUM_LAYERS,\n    F.relu,\n    DROPOUT,\n)\nmodel = model.to(device)\nloss_fcn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T18:09:40.472703Z","iopub.execute_input":"2025-04-03T18:09:40.472934Z","iopub.status.idle":"2025-04-03T18:09:41.996270Z","shell.execute_reply.started":"2025-04-03T18:09:40.472914Z","shell.execute_reply":"2025-04-03T18:09:41.995331Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"avg = 0\niter_tput = []\nbest_eval_acc = 0\nbest_test_acc = 0\nfor epoch in range(NUM_EPOCHS):\n    tic = time.time()\n\n    # Loop over the dataloader to sample the computation dependency graph as a list of\n    # blocks.\n    for step, (input_nodes, seeds, blocks) in enumerate(dataloader):\n        tic_step = time.time()\n\n        # copy block to gpu\n        blocks = [blk.int().to(device) for blk in blocks]\n\n        # Load the input features as well as output labels\n        batch_inputs, batch_labels = load_subtensor(\n            nfeat, labels, seeds, input_nodes\n        )\n\n        # Compute loss and prediction\n        batch_pred = model(blocks, batch_inputs)\n        loss = loss_fcn(batch_pred, batch_labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        iter_tput.append(len(seeds) / (time.time() - tic_step))\n        if step % LOG_EVERY == 0:\n            acc = compute_acc(batch_pred, batch_labels)\n            gpu_mem_alloc = (\n                th.cuda.max_memory_allocated() / 1000000\n                if th.cuda.is_available()\n                else 0\n            )\n            print(\n                \"Epoch {:05d} | Step {:05d} | Loss {:.4f} | Train Acc {:.4f} | Speed (samples/sec) {:.4f} | GPU {:.1f} MB\".format(\n                    epoch,\n                    step,\n                    loss.item(),\n                    acc.item(),\n                    np.mean(iter_tput[3:]),\n                    gpu_mem_alloc,\n                )\n            )\n        gc.collect()\n\n    toc = time.time()\n    print(\"Epoch Time(s): {:.4f}\".format(toc - tic))\n    if epoch >= 5:\n        avg += toc - tic\n    if epoch % EVAL_EVERY == 0 and epoch != 0:\n        eval_acc, test_acc, pred = evaluate(\n            model, g, nfeat, labels, val_nid, test_nid, device\n        )\n        if SAVE_PRED:\n            np.savetxt(\n                SAVE_PRED + \"%02d\" % epoch,\n                pred.argmax(1).cpu().numpy(),\n                \"%d\",\n            )\n        print(\"Eval Acc {:.4f}\".format(eval_acc))\n        if eval_acc > best_eval_acc:\n            best_eval_acc = eval_acc\n            best_test_acc = test_acc\n        print(\n            \"Best Eval Acc {:.4f} Test Acc {:.4f}\".format(\n                best_eval_acc, best_test_acc\n            )\n        )\n    gc.collect()\n\nprint(\"Avg epoch time: {}\".format(avg / (epoch - 4)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T18:09:41.997275Z","iopub.execute_input":"2025-04-03T18:09:41.997920Z","iopub.status.idle":"2025-04-03T18:25:29.782170Z","shell.execute_reply.started":"2025-04-03T18:09:41.997893Z","shell.execute_reply":"2025-04-03T18:25:29.781390Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/dgl/dataloading/dataloader.py:1144: DGLWarning: Dataloader CPU affinity opt is not enabled, consider switching it on (see enable_cpu_affinity() or CPU best practices for DGL [https://docs.dgl.ai/tutorials/cpu/cpu_best_practises.html])\n  dgl_warning(\n/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 00000 | Step 00000 | Loss 7.7536 | Train Acc 0.0280 | Speed (samples/sec) nan | GPU 1707.1 MB\nEpoch 00000 | Step 00020 | Loss 1.5385 | Train Acc 0.6820 | Speed (samples/sec) 54367.3826 | GPU 1715.9 MB\nEpoch 00000 | Step 00040 | Loss 0.8794 | Train Acc 0.7780 | Speed (samples/sec) 54763.7308 | GPU 1716.2 MB\nEpoch 00000 | Step 00060 | Loss 0.7643 | Train Acc 0.8150 | Speed (samples/sec) 55251.6711 | GPU 1717.2 MB\nEpoch 00000 | Step 00080 | Loss 0.5721 | Train Acc 0.8480 | Speed (samples/sec) 56242.9250 | GPU 1717.2 MB\nEpoch 00000 | Step 00100 | Loss 0.6614 | Train Acc 0.8380 | Speed (samples/sec) 56602.8436 | GPU 1717.2 MB\nEpoch 00000 | Step 00120 | Loss 0.6695 | Train Acc 0.8240 | Speed (samples/sec) 56811.7797 | GPU 1717.2 MB\nEpoch 00000 | Step 00140 | Loss 0.5424 | Train Acc 0.8600 | Speed (samples/sec) 56841.8892 | GPU 1717.2 MB\nEpoch 00000 | Step 00160 | Loss 0.5339 | Train Acc 0.8610 | Speed (samples/sec) 56401.1302 | GPU 1717.2 MB\nEpoch 00000 | Step 00180 | Loss 0.4671 | Train Acc 0.8750 | Speed (samples/sec) 56086.8555 | GPU 1717.2 MB\nEpoch Time(s): 42.9263\nEpoch 00001 | Step 00000 | Loss 0.5267 | Train Acc 0.8630 | Speed (samples/sec) 55573.0466 | GPU 1717.2 MB\nEpoch 00001 | Step 00020 | Loss 0.4504 | Train Acc 0.8790 | Speed (samples/sec) 54884.6486 | GPU 1717.2 MB\nEpoch 00001 | Step 00040 | Loss 0.5073 | Train Acc 0.8620 | Speed (samples/sec) 54711.4757 | GPU 1717.2 MB\nEpoch 00001 | Step 00060 | Loss 0.4456 | Train Acc 0.8860 | Speed (samples/sec) 54394.1583 | GPU 1717.2 MB\nEpoch 00001 | Step 00080 | Loss 0.5274 | Train Acc 0.8650 | Speed (samples/sec) 53931.6110 | GPU 1717.2 MB\nEpoch 00001 | Step 00100 | Loss 0.4070 | Train Acc 0.8820 | Speed (samples/sec) 53463.6923 | GPU 1717.2 MB\nEpoch 00001 | Step 00120 | Loss 0.4768 | Train Acc 0.8730 | Speed (samples/sec) 53487.6639 | GPU 1717.2 MB\nEpoch 00001 | Step 00140 | Loss 0.3884 | Train Acc 0.8920 | Speed (samples/sec) 53161.0209 | GPU 1717.2 MB\nEpoch 00001 | Step 00160 | Loss 0.4698 | Train Acc 0.8920 | Speed (samples/sec) 53303.0983 | GPU 1717.2 MB\nEpoch 00001 | Step 00180 | Loss 0.3924 | Train Acc 0.8910 | Speed (samples/sec) 53061.3084 | GPU 1717.2 MB\nEpoch Time(s): 47.0062\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:19<00:00, 125.94it/s]\n100%|██████████| 2450/2450 [00:18<00:00, 133.97it/s]\n100%|██████████| 2450/2450 [00:19<00:00, 125.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.8960\nBest Eval Acc 0.8960 Test Acc 0.7515\nEpoch 00002 | Step 00000 | Loss 0.3342 | Train Acc 0.9060 | Speed (samples/sec) 52780.7229 | GPU 6345.1 MB\nEpoch 00002 | Step 00020 | Loss 0.4045 | Train Acc 0.8920 | Speed (samples/sec) 52847.3061 | GPU 6345.1 MB\nEpoch 00002 | Step 00040 | Loss 0.4167 | Train Acc 0.8860 | Speed (samples/sec) 52822.1809 | GPU 6345.1 MB\nEpoch 00002 | Step 00060 | Loss 0.3190 | Train Acc 0.8970 | Speed (samples/sec) 52841.8415 | GPU 6345.1 MB\nEpoch 00002 | Step 00080 | Loss 0.4317 | Train Acc 0.8840 | Speed (samples/sec) 52988.0429 | GPU 6345.1 MB\nEpoch 00002 | Step 00100 | Loss 0.3953 | Train Acc 0.9050 | Speed (samples/sec) 52888.8023 | GPU 6345.1 MB\nEpoch 00002 | Step 00120 | Loss 0.3754 | Train Acc 0.8910 | Speed (samples/sec) 53217.1732 | GPU 6345.1 MB\nEpoch 00002 | Step 00140 | Loss 0.3351 | Train Acc 0.9010 | Speed (samples/sec) 53536.7669 | GPU 6345.1 MB\nEpoch 00002 | Step 00160 | Loss 0.3869 | Train Acc 0.8930 | Speed (samples/sec) 53625.0842 | GPU 6345.1 MB\nEpoch 00002 | Step 00180 | Loss 0.5081 | Train Acc 0.8770 | Speed (samples/sec) 54007.1083 | GPU 6345.1 MB\nEpoch Time(s): 42.1038\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:18<00:00, 134.91it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 140.28it/s]\n100%|██████████| 2450/2450 [00:18<00:00, 129.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9031\nBest Eval Acc 0.9031 Test Acc 0.7618\nEpoch 00003 | Step 00000 | Loss 0.4122 | Train Acc 0.8960 | Speed (samples/sec) 54209.6085 | GPU 6805.5 MB\nEpoch 00003 | Step 00020 | Loss 0.4336 | Train Acc 0.8940 | Speed (samples/sec) 54344.6957 | GPU 6805.5 MB\nEpoch 00003 | Step 00040 | Loss 0.3426 | Train Acc 0.9120 | Speed (samples/sec) 54307.6658 | GPU 6805.5 MB\nEpoch 00003 | Step 00060 | Loss 0.3611 | Train Acc 0.8950 | Speed (samples/sec) 54351.4735 | GPU 6805.5 MB\nEpoch 00003 | Step 00080 | Loss 0.4443 | Train Acc 0.8700 | Speed (samples/sec) 54359.5051 | GPU 6805.5 MB\nEpoch 00003 | Step 00100 | Loss 0.3743 | Train Acc 0.8950 | Speed (samples/sec) 54283.8393 | GPU 6805.5 MB\nEpoch 00003 | Step 00120 | Loss 0.3398 | Train Acc 0.9100 | Speed (samples/sec) 54292.4933 | GPU 6805.5 MB\nEpoch 00003 | Step 00140 | Loss 0.3330 | Train Acc 0.9040 | Speed (samples/sec) 54311.0523 | GPU 6805.5 MB\nEpoch 00003 | Step 00160 | Loss 0.4378 | Train Acc 0.8880 | Speed (samples/sec) 54530.5123 | GPU 6805.5 MB\nEpoch 00003 | Step 00180 | Loss 0.3512 | Train Acc 0.8940 | Speed (samples/sec) 54621.3978 | GPU 6805.5 MB\nEpoch Time(s): 42.0389\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:17<00:00, 141.90it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 142.26it/s]\n100%|██████████| 2450/2450 [00:18<00:00, 132.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9079\nBest Eval Acc 0.9079 Test Acc 0.7799\nEpoch 00004 | Step 00000 | Loss 0.3688 | Train Acc 0.8930 | Speed (samples/sec) 54629.7633 | GPU 6813.0 MB\nEpoch 00004 | Step 00020 | Loss 0.3489 | Train Acc 0.9150 | Speed (samples/sec) 54446.3986 | GPU 6813.0 MB\nEpoch 00004 | Step 00040 | Loss 0.3580 | Train Acc 0.9000 | Speed (samples/sec) 54386.8240 | GPU 6813.0 MB\nEpoch 00004 | Step 00060 | Loss 0.3703 | Train Acc 0.8900 | Speed (samples/sec) 54217.2549 | GPU 6813.0 MB\nEpoch 00004 | Step 00080 | Loss 0.3176 | Train Acc 0.9250 | Speed (samples/sec) 54030.6511 | GPU 6813.0 MB\nEpoch 00004 | Step 00100 | Loss 0.3040 | Train Acc 0.9120 | Speed (samples/sec) 53953.8362 | GPU 6813.0 MB\nEpoch 00004 | Step 00120 | Loss 0.3534 | Train Acc 0.9030 | Speed (samples/sec) 53805.2100 | GPU 6813.0 MB\nEpoch 00004 | Step 00140 | Loss 0.3284 | Train Acc 0.9000 | Speed (samples/sec) 53702.4588 | GPU 6813.0 MB\nEpoch 00004 | Step 00160 | Loss 0.3306 | Train Acc 0.8910 | Speed (samples/sec) 53602.8774 | GPU 6813.0 MB\nEpoch 00004 | Step 00180 | Loss 0.2971 | Train Acc 0.9100 | Speed (samples/sec) 53552.4505 | GPU 6813.0 MB\nEpoch Time(s): 42.9817\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:17<00:00, 141.84it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 139.26it/s]\n100%|██████████| 2450/2450 [00:18<00:00, 130.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9103\nBest Eval Acc 0.9103 Test Acc 0.7775\nEpoch 00005 | Step 00000 | Loss 0.3718 | Train Acc 0.9000 | Speed (samples/sec) 53508.4020 | GPU 6813.0 MB\nEpoch 00005 | Step 00020 | Loss 0.3177 | Train Acc 0.9070 | Speed (samples/sec) 53353.2445 | GPU 6813.0 MB\nEpoch 00005 | Step 00040 | Loss 0.3198 | Train Acc 0.9080 | Speed (samples/sec) 53239.3748 | GPU 6813.0 MB\nEpoch 00005 | Step 00060 | Loss 0.3043 | Train Acc 0.9140 | Speed (samples/sec) 53123.6160 | GPU 6813.0 MB\nEpoch 00005 | Step 00080 | Loss 0.3229 | Train Acc 0.9110 | Speed (samples/sec) 52974.9239 | GPU 6813.0 MB\nEpoch 00005 | Step 00100 | Loss 0.3312 | Train Acc 0.9070 | Speed (samples/sec) 52942.3722 | GPU 6813.0 MB\nEpoch 00005 | Step 00120 | Loss 0.3304 | Train Acc 0.9040 | Speed (samples/sec) 52907.2196 | GPU 6813.0 MB\nEpoch 00005 | Step 00140 | Loss 0.2773 | Train Acc 0.9220 | Speed (samples/sec) 52871.6230 | GPU 6813.0 MB\nEpoch 00005 | Step 00160 | Loss 0.3134 | Train Acc 0.9090 | Speed (samples/sec) 52813.7381 | GPU 6813.0 MB\nEpoch 00005 | Step 00180 | Loss 0.3281 | Train Acc 0.9120 | Speed (samples/sec) 52729.1775 | GPU 6813.0 MB\nEpoch Time(s): 43.1338\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:17<00:00, 136.68it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 136.62it/s]\n100%|██████████| 2450/2450 [00:18<00:00, 129.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9115\nBest Eval Acc 0.9115 Test Acc 0.7843\nEpoch 00006 | Step 00000 | Loss 0.2789 | Train Acc 0.9220 | Speed (samples/sec) 52705.3740 | GPU 6813.0 MB\nEpoch 00006 | Step 00020 | Loss 0.2882 | Train Acc 0.9170 | Speed (samples/sec) 52580.1775 | GPU 6813.0 MB\nEpoch 00006 | Step 00040 | Loss 0.3946 | Train Acc 0.8870 | Speed (samples/sec) 52519.8520 | GPU 6813.0 MB\nEpoch 00006 | Step 00060 | Loss 0.3104 | Train Acc 0.9140 | Speed (samples/sec) 52547.9508 | GPU 6813.0 MB\nEpoch 00006 | Step 00080 | Loss 0.2865 | Train Acc 0.9160 | Speed (samples/sec) 52549.8592 | GPU 6813.0 MB\nEpoch 00006 | Step 00100 | Loss 0.3476 | Train Acc 0.9100 | Speed (samples/sec) 52488.4733 | GPU 6813.0 MB\nEpoch 00006 | Step 00120 | Loss 0.3690 | Train Acc 0.9040 | Speed (samples/sec) 52425.0250 | GPU 6813.0 MB\nEpoch 00006 | Step 00140 | Loss 0.3428 | Train Acc 0.9020 | Speed (samples/sec) 52364.6559 | GPU 6813.0 MB\nEpoch 00006 | Step 00160 | Loss 0.3615 | Train Acc 0.9040 | Speed (samples/sec) 52273.4197 | GPU 6813.0 MB\nEpoch 00006 | Step 00180 | Loss 0.3964 | Train Acc 0.8980 | Speed (samples/sec) 52205.9859 | GPU 6813.0 MB\nEpoch Time(s): 43.2735\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:17<00:00, 141.35it/s]\n100%|██████████| 2450/2450 [00:17<00:00, 136.93it/s]\n100%|██████████| 2450/2450 [00:18<00:00, 131.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9135\nBest Eval Acc 0.9135 Test Acc 0.7857\nEpoch 00007 | Step 00000 | Loss 0.3132 | Train Acc 0.9090 | Speed (samples/sec) 52193.6359 | GPU 6813.0 MB\nEpoch 00007 | Step 00020 | Loss 0.3485 | Train Acc 0.9150 | Speed (samples/sec) 52126.6800 | GPU 6813.0 MB\nEpoch 00007 | Step 00040 | Loss 0.3081 | Train Acc 0.9090 | Speed (samples/sec) 52138.2312 | GPU 6813.0 MB\nEpoch 00007 | Step 00060 | Loss 0.2775 | Train Acc 0.9200 | Speed (samples/sec) 52188.0122 | GPU 6813.0 MB\nEpoch 00007 | Step 00080 | Loss 0.3191 | Train Acc 0.9020 | Speed (samples/sec) 52141.6796 | GPU 6813.0 MB\nEpoch 00007 | Step 00100 | Loss 0.3272 | Train Acc 0.9010 | Speed (samples/sec) 52153.4474 | GPU 6813.0 MB\nEpoch 00007 | Step 00120 | Loss 0.3294 | Train Acc 0.9110 | Speed (samples/sec) 52184.5761 | GPU 6813.0 MB\nEpoch 00007 | Step 00140 | Loss 0.2758 | Train Acc 0.9220 | Speed (samples/sec) 52161.5496 | GPU 6813.0 MB\nEpoch 00007 | Step 00160 | Loss 0.2668 | Train Acc 0.9230 | Speed (samples/sec) 52113.9555 | GPU 6813.0 MB\nEpoch 00007 | Step 00180 | Loss 0.3138 | Train Acc 0.9160 | Speed (samples/sec) 52099.7820 | GPU 6813.0 MB\nEpoch Time(s): 41.6526\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:16<00:00, 146.27it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 145.18it/s]\n100%|██████████| 2450/2450 [00:18<00:00, 133.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9141\nBest Eval Acc 0.9141 Test Acc 0.7871\nEpoch 00008 | Step 00000 | Loss 0.2886 | Train Acc 0.9200 | Speed (samples/sec) 52081.9199 | GPU 6813.0 MB\nEpoch 00008 | Step 00020 | Loss 0.3072 | Train Acc 0.9080 | Speed (samples/sec) 52078.0135 | GPU 6813.0 MB\nEpoch 00008 | Step 00040 | Loss 0.3149 | Train Acc 0.9100 | Speed (samples/sec) 52064.6879 | GPU 6813.0 MB\nEpoch 00008 | Step 00060 | Loss 0.3358 | Train Acc 0.9070 | Speed (samples/sec) 52106.6597 | GPU 6813.0 MB\nEpoch 00008 | Step 00080 | Loss 0.3293 | Train Acc 0.9040 | Speed (samples/sec) 52022.2845 | GPU 6813.0 MB\nEpoch 00008 | Step 00100 | Loss 0.2550 | Train Acc 0.9320 | Speed (samples/sec) 52000.0891 | GPU 6813.0 MB\nEpoch 00008 | Step 00120 | Loss 0.3787 | Train Acc 0.8930 | Speed (samples/sec) 51956.3177 | GPU 6813.0 MB\nEpoch 00008 | Step 00140 | Loss 0.3461 | Train Acc 0.9080 | Speed (samples/sec) 51921.6744 | GPU 6813.0 MB\nEpoch 00008 | Step 00160 | Loss 0.3253 | Train Acc 0.9070 | Speed (samples/sec) 51860.9294 | GPU 6813.0 MB\nEpoch 00008 | Step 00180 | Loss 0.3406 | Train Acc 0.9000 | Speed (samples/sec) 51823.3852 | GPU 6813.0 MB\nEpoch Time(s): 41.6285\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:17<00:00, 143.36it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 145.60it/s]\n100%|██████████| 2450/2450 [00:18<00:00, 134.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9157\nBest Eval Acc 0.9157 Test Acc 0.7907\nEpoch 00009 | Step 00000 | Loss 0.3104 | Train Acc 0.9030 | Speed (samples/sec) 51817.2121 | GPU 6813.0 MB\nEpoch 00009 | Step 00020 | Loss 0.3491 | Train Acc 0.9060 | Speed (samples/sec) 51796.6883 | GPU 6813.0 MB\nEpoch 00009 | Step 00040 | Loss 0.2841 | Train Acc 0.9140 | Speed (samples/sec) 51777.2843 | GPU 6813.0 MB\nEpoch 00009 | Step 00060 | Loss 0.2798 | Train Acc 0.9200 | Speed (samples/sec) 51703.0830 | GPU 6813.0 MB\nEpoch 00009 | Step 00080 | Loss 0.2531 | Train Acc 0.9250 | Speed (samples/sec) 51635.9527 | GPU 6813.0 MB\nEpoch 00009 | Step 00100 | Loss 0.2967 | Train Acc 0.9180 | Speed (samples/sec) 51658.3196 | GPU 6813.0 MB\nEpoch 00009 | Step 00120 | Loss 0.2819 | Train Acc 0.9110 | Speed (samples/sec) 51617.8213 | GPU 6813.0 MB\nEpoch 00009 | Step 00140 | Loss 0.3481 | Train Acc 0.8990 | Speed (samples/sec) 51632.3964 | GPU 6813.0 MB\nEpoch 00009 | Step 00160 | Loss 0.3124 | Train Acc 0.9090 | Speed (samples/sec) 51634.1312 | GPU 6813.0 MB\nEpoch 00009 | Step 00180 | Loss 0.2900 | Train Acc 0.9220 | Speed (samples/sec) 51638.8909 | GPU 6813.0 MB\nEpoch Time(s): 40.9870\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2450/2450 [00:16<00:00, 146.75it/s]\n100%|██████████| 2450/2450 [00:16<00:00, 145.20it/s]\n100%|██████████| 2450/2450 [00:18<00:00, 134.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Eval Acc 0.9171\nBest Eval Acc 0.9171 Test Acc 0.7901\nAvg epoch time: 42.135081911087035\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\ntorch.save(model, './SAGE.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T18:29:14.107535Z","iopub.execute_input":"2025-04-03T18:29:14.107932Z","iopub.status.idle":"2025-04-03T18:29:14.117202Z","shell.execute_reply.started":"2025-04-03T18:29:14.107898Z","shell.execute_reply":"2025-04-03T18:29:14.116159Z"}},"outputs":[],"execution_count":13}]}